{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa886d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unsupervised Medical Topic Discovery: Exploration & Visualization\n",
    "\n",
    "# --- Imports and Setup ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom utility functions\n",
    "# Make sure you are running this notebook from the 'medical-topic-discovery' root or have\n",
    "# added the 'src/' directory to your path.\n",
    "import sys\n",
    "sys.path.append('../src') \n",
    "from utils import preprocess_text, display_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "N_TOPICS = 4  \n",
    "N_TOP_WORDS = 8\n",
    "FILE_PATH = '../data/medical_corpus.txt'\n",
    "\n",
    "# Download NLTK resources (needed for stopwords and lemmatization)\n",
    "print(\"Downloading NLTK data (if not already present)...\")\n",
    "try:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    print(\"NLTK downloads complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK data: {e}. Please check your internet connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # 1. Data Acquisition and Inspection\n",
    "print(f\"\\nLoading data from: {FILE_PATH}\")\n",
    "try:\n",
    "    with open(FILE_PATH, 'r') as f:\n",
    "        corpus = f.read().splitlines()\n",
    "    print(f\"Loaded {len(corpus)} documents.\")\n",
    "    \n",
    "    # Convert to DataFrame for easier inspection\n",
    "    df = pd.DataFrame({'document': corpus})\n",
    "    display(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Data file not found at {FILE_PATH}. Please check the path.\")\n",
    "\n",
    "# # 2. Preprocessing\n",
    "print(\"\\nApplying preprocessing (cleaning, stop word removal, lemmatization)...\")\n",
    "df['cleaned_document'] = df['document'].apply(preprocess_text)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # 3. Feature Engineering (Count Vectorization)\n",
    "print(\"\\nCreating the Document-Term Matrix (DTM) with CountVectorizer...\")\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2) # Tweak these parameters to refine features\n",
    "data_vectorized = vectorizer.fit_transform(df['cleaned_document'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"DTM Shape: {data_vectorized.shape} (Documents x Features/Words)\")\n",
    "\n",
    "\n",
    "# # 4. Topic Modeling (LDA)\n",
    "print(f\"\\nTraining Latent Dirichlet Allocation (LDA) model with K={N_TOPICS} topics...\")\n",
    "\n",
    "# Initialize and train the LDA model\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=N_TOPICS,\n",
    "    max_iter=10, # Increased iterations for better convergence in notebook\n",
    "    learning_method='online',\n",
    "    random_state=42,\n",
    "    n_jobs=-1 # Use all processors\n",
    ")\n",
    "lda.fit(data_vectorized)\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Interpretation: Print the top words for each discovered topic\n",
    "display_topics(lda, feature_names, N_TOP_WORDS)\n",
    "\n",
    "\n",
    "# # 5. Visualization using Dimensionality Reduction (PCA)\n",
    "\n",
    "# 5.1 Transform the vectorized data to get Document-Topic distribution\n",
    "doc_topic_distribution = lda.transform(data_vectorized)\n",
    "df['main_topic'] = np.argmax(doc_topic_distribution, axis=1)\n",
    "\n",
    "# 5.2 Reduce the *Document-Term Matrix* to 2 dimensions using PCA\n",
    "# We use PCA for simplicity in this exploration step\n",
    "print(\"\\nReducing DTM dimensions using PCA for visualization...\")\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "data_pca = pca.fit_transform(data_vectorized.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "plot_df = pd.DataFrame({\n",
    "    'PC1': data_pca[:, 0],\n",
    "    'PC2': data_pca[:, 1],\n",
    "    'Topic': df['main_topic']\n",
    "})\n",
    "\n",
    "# 5.3 Plotting the Topics\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    x='PC1', \n",
    "    y='PC2', \n",
    "    hue='Topic', \n",
    "    data=plot_df, \n",
    "    palette='Spectral', \n",
    "    s=100, \n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.title(f'Document Clustering based on LDA Topic Assignment (Reduced via PCA)')\n",
    "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "plt.legend(title='Discovered Topic Index')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization complete. Clear separation in the plot indicates strong topic discovery!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
